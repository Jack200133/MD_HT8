---
title: "MD_HDT8"
---

```{r}
set.seed(123)
datos <- read.csv("train.csv")
```

```{r echo=F, include=F, load_libraries}
library(dplyr)
library(hopkins)
library(factoextra)
library(ggrepel)
library(cluster)
library(flexclust)
library(FeatureImpCluster)
library(stringr)
library(tidyr)
library(stats)
library(graphics)
library(NbClust)
library(mclust)
library(GGally)
library(corrplot)
library(caret)
library(ggplot2)
library(kableExtra)
library(e1071)
library(rpart)
library(rpart.plot)
library(naivebayes)
library(randomForest)
library(dummy)
library(profvis)
library(mlr)
library(Metrics)
library(nnet)
library(neuralnet)
```

## 1. Dvisión de variables numéricas y obtención de data de prueba y entrenamiento

### 1.1 Transformación y división de variables

Al observar las variables se puede evidenciar que hay diferentes variables que tienen datos en diferentes escalas. Además, del análisis exploratorio previo se sabe que las variables no siguen distribuciones normales, así que se escalaron y normalizaron las variables.

```{r echo=FALSE}
head(datos, )

multi_variables <- c("SalePrice", "OverallQual", "MasVnrArea", "BsmtFinSF1", "GrLivArea", "Fireplaces", "WoodDeckSF", "OpenPorchSF", "TotalBsmtSF")

num2 <- datos[, multi_variables]
datos1 <- datos[complete.cases(num2), ]
datos1 <- mutate_if(datos1, is.numeric, scale)
datos1 <- datos1[, multi_variables]


variables_m2 <- c("OverallQual", "BsmtFinSF1", "BsmtUnfSF", "X2ndFlrSF", "PoolArea", "MiscVal", "Neighborhood", "HouseStyle", "LotConfig", "SalePrice")

numeric_variables <- c("OverallQual", "BsmtFinSF1", "BsmtUnfSF", "X2ndFlrSF", "PoolArea", "MiscVal", "SalePrice")

datos <- datos[, multi_variables]
cualitativas <- datos[, !(names(datos) %in% multi_variables)]
cualitativas <- cualitativas[, !(names(cualitativas) %in% c("Id"))]

datos <- datos %>% mutate_at(colnames(cualitativas), function(x) as.factor(x))

numericas <- datos[, multi_variables]
datos <- datos[complete.cases(numericas), ]
numericas <- na.omit(numericas)
numericas_norm <- mutate_if(numericas, is.numeric, scale)
numericas_norm <- scale(numericas_norm)
datos <- data.frame(numericas_norm, datos[, -match(multi_variables, names(datos))])
```


### 1.2. Creación de clasificación de la variable de precios
```{r}
p33 <- quantile(datos$SalePrice, 0.33)
p66 <- quantile(datos$SalePrice, 0.66)

datos <- datos %>%
      mutate(clasificacion = ifelse(datos$SalePrice < p33, "Economicas",
            ifelse(datos$SalePrice < p66, "Intermedias",
                  "Caras"
            )
      ))
datos$clasificacion <- as.factor(datos$clasificacion)
```


### 2. Uso de train y test previos

```{r}
head(datos)
```

```{r}
porcentaje <- 0.7
set.seed(123)

corte <- sample(nrow(datos), nrow(datos) * porcentaje)
train <- datos[corte, ]
test <- datos[-corte, ]
train <- select(train, -SalePrice)
test <- select(test, -SalePrice)
```


## 3. Generar dos modelos de redes neuronales, predicciones y matrices.
 
### 3.1 Primer modelo

```{r}
Rprof(memory.profiling = TRUE)
modelo1 <- caret::train(clasificacion ~ ., data = train, method = "nnet", trace = F, tuneGrid = expand.grid(size = 4, decay = 0.1), nnet = list(droput = 0.5), activation = "logistic")
Rprof(NULL)
pm1 <- summaryRprof(memory = "both")
```


## 3.2 Segundo modelo

```{r}
Rprof(memory.profiling = TRUE)
modelo2 <- caret::train(clasificacion ~ ., data = train, method = "nnet", trace = F, tuneGrid = expand.grid(size = 16, decay = 0.1), nnet = list(droput = 0.5), activation = "sigmoid")
Rprof(NULL)
pm2 <- summaryRprof(memory = "both")
```

## 4. Predicciones

### 4.1 Predicción con primer modelo

```{r}
prediccion1 <- predict(modelo1, newdata = test)
cfm1 <- confusionMatrix(prediccion1, test$clasificacion)
```

### 4.2 Predicción con segundo modelo

```{r}
prediccion2 <- predict(modelo2, newdata = test)
cfm2 <- confusionMatrix(prediccion2, test$clasificacion)
```

## 5. Matrices de confusión de modelos

### 5.1 Matriz de primer modelo
```{r}
cfm1
```


### 5.2 Matriz de segundo modelo
La matriz para el segundo modelo
```{r}
cfm2
```


## 6 Comparacion de resultados

### Red neuronal 1
Este modelo obtuvo un accurracy de 0.754
Este modelo obtuvo un tiempo de procesamiento de `r pm1$sampling.time`
El modelo obtuvo un sensitivity de 0.855 y un specificity de 0.899 indicando asi que modelo no tiene tantas equivocaciones.


### Red neuronal 2
Este modelo obtuvo un accurracy de 0.719
Este modelo obtuvo un tiempo de procesamiento de `r pm2$sampling.time`
El modelo obtuvo un sensitivity de 0.789 y un specificity de 0.919 indicando asi que modelo no tiene tantas equivocaciones.


### Conclusion

Segun los datos que se observan anteriormente se puede definir que el mejor modelo de las dos redes neuronales es la primera, la cual tiene una solo una capa de 16 neuronas.

## 7. Análisis de overfitting

### 7.1 Primer modelo

```{r warning=FALSE, message=FALSE}
    datos.task = makeClassifTask(data = train, target = "clasificacion")
    rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
    lrn = makeLearner("classif.nnet", size = 4, decay = 1e-4, maxit = 1000, trace = FALSE)
    lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                      percs = seq(0.1, 1, by = 0.1),
                                      measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                      show.info = FALSE)
      plotLearningCurve(lc2, facet = "learner")
```

Tras observar la curva de aprendizaje, se puede notar que la curva de training siempre va en ascenso, mientras mayor cantidad de datos mayor es la curva lo que indica que el modelo no posee un infra ajuste. Por otro lado, al observar la curva de test se nota que al final con el último grupo de datos en lugar de que la curva disminuye va en aumento lo que no debe de pasar, indicando así que el modelo posee sobreajuste. Además, se puede reforzar esta conclusión al notar que las dos curvas nunca llegan a converger y la distancia entre ellas es muy amplia.

### 7.2 Segundo modelo

```{r warning=FALSE, message=FALSE}
    datos.task = makeClassifTask(data = train, target = "clasificacion")
    rin2 = makeResampleDesc(method = "CV", iters = 10, predict = "both")
    lrn = makeLearner("classif.nnet", size = 16, decay = 1e-4, maxit = 1000, trace = FALSE)
    lc2 = generateLearningCurveData(learners = lrn, task = datos.task,
                                    percs = seq(0.1, 1, by = 0.1),
                                    measures = list(ber, setAggregation(ber, train.mean)), resampling = rin2,
                                      show.info = FALSE)
      plotLearningCurve(lc2, facet = "learner")
```
  
El segundo modelo se evidencia en su curva de aprendizaje que tiene overfitting. Se aprecia que la curva de entrenamiento sube cuando se tienen más datos, lo cual indica que el modelo se está ajustando mucho a ellos. Además, existe un espacio demasiado grande entre curvas. Este sobreajuste se puede deber a que la complejidad del modelo es demasiado alta para los datos que se tienen. También es posible que se tengan muy pocos datos y/o features.


## 8. Tuneo de parametros

```{r }
grid <- expand.grid(size = c(2, 4, 6, 10),
                    decay = c(0.01, 0.1, 0.5, 1.5, 1.25))

modelo_tuneado <- caret::train(clasificacion ~ ., 
                               data = train, 
                                 method = "nnet", 
                                 trace = F, 
                               tuneGrid = grid, 
                               nnet = list(droput = 0.5), 
                               maxit = 100)
  
modelo_tuneado$bestTune
summary(modelo_tuneado)
  
```
  
```{r echo=F, message=F}

library(mlr)

set.seed(123) # Establecer una semilla para la reproducibilidad
train_indices <- sample(1:nrow(test), size = floor(0.5 * nrow(test)))
train_50 <- train[train_indices, ]
valid_50 <- train[-train_indices, ]

# Crear la tarea de clasificación
datos.task <- makeClassifTask(data = train_50, target = "clasificacion")

# Crear el descriptor de muestreo
resampling <- makeResampleDesc("CV", iters = 10)

# Crear el aprendiz con el algoritmo deseado
learner <- makeLearner("classif.nnet", predict.type = "prob")

# Definir los rangos de hiperparámetros
param_set <- makeParamSet(
  makeDiscreteParam("size", values = c(2, 4, 6, 8, 10)),
  makeNumericParam("decay", lower = 1e-5, upper = 1e-1, trafo = function(x) 10^x)
)

# Definir la estrategia de búsqueda en cuadrícula
control <- makeTuneControlGrid()

#   Configurar la búsqueda de hiperparámetros
tune_params <- makeTuneWrapper(learner, resampling, measures = list(ber), par.set = param_set, control = control)

# Ejecutar la búsqueda de hiperparámetros
res <- resample(tune_params, datos.task, resampling, measures = list(ber), extract = getTuneResult)

# Mostrar los resultados
print(res)

# Obtener el mejor conjunto de hiperparámetros
best_params <- res$extract[[1]]$x
print(best_params)

```

Al utilizar crossvalidation con parameter tuning, se obtuvo que la mejor cantidad de neuronas es 10 y el mejor decay es 1.26 aproximadamente. No se muestra porque la ejecución del código es muy extensa. A pesar de que estos valores puede que mejoren un poco el accuracy, el overfitting es muy probable que se mantenga.

### 8.2 Modelo Tuneado

```{r}
Rprof(memory.profiling = TRUE)
modelo3 <- caret::train(clasificacion ~ ., data = train, method = "nnet", trace = F, tuneGrid = expand.grid(size = 10, decay = 1.258925), nnet = list(droput = 0.5), activation = "logistic")
Rprof(NULL)
pm3 <- summaryRprof(memory = "both")
```
```{r}
prediccion3 <- predict(modelo3, newdata = test)
cfm3 <- confusionMatrix(prediccion3, test$clasificacion)
```

```{r}
cfm3
```

Se obtuvo un accuracy de 0.77, solo un 0.01 arriba del mejor accuracy anterior. Por lo que no se tiene una diferencia tan significativa para el modelo. Es casi seguro que el overfitting se mantuvo.


## 9. Selección de SalePrice como variable respuesta

```{r}
porcentaje <- 0.7
set.seed(123)

corte <- sample(nrow(datos), nrow(datos) * porcentaje)
train <- datos[corte, ]
test <- datos[-corte, ]
train <- train[, multi_variables]
test <- test[, multi_variables]
```

## 10. Modelos de regresión lineal con redes neuronales.

### 10.1 Primer modelo

```{r}
Rprof(memory.profiling = TRUE)
model1 <- neuralnet(SalePrice~., train, hidden = 2, rep = 3, act.fct = "logistic",linear.output = TRUE )
Rprof(NULL)
pm1 <- summaryRprof(memory = "both")
```

### 10.2 Segundo modelo

```{r}
Rprof(memory.profiling = TRUE)
model2 <- neuralnet(SalePrice ~ ., data=train, threshold=0.0001, act.fct = "tanh",err.fct = "sse",linear.output = TRUE )
Rprof(NULL)
pm2 <- summaryRprof(memory = "both")
```

## 11. Predicción

### 11.1 Predicción con primer modelo

```{r}
prediction_1 <- predict(model1, test)
MSE <- mean((test$SalePrice - prediction_1)^2)
RMSE <- sqrt(MSE)
SSR <- sum((test$SalePrice- prediction_1)^2)
SST <- sum((test$SalePrice - mean(test$SalePrice))^2)
prediction_1_train <- round(predict(model1, newdata = train), 0)
R_squared <- 1 - (SSR / SST)
MSE
RMSE
R_squared
```

### 11.2 Predicción con segundo modelo

```{r}
prediction_2 <- predict(model2, test)
MSE <- mean((test$SalePrice - prediction_2)^2)
RMSE <- sqrt(MSE)
SSR <- sum((test$SalePrice- prediction_2)^2)
SST <- sum((test$SalePrice - mean(test$SalePrice))^2)
prediction_2_train <- round(predict(model2, newdata = train), 0)
R_squared <- 1 - (SSR / SST)
MSE
RMSE
R_squared
```


Se puede observar que ambos modelos fueron bastante buenos para realizar la predicción de los precios de las casas. Se tuvo un bajo MSE y RMSE en ambos modelos. El mejor modelo fue el segundo, teniendo 0.02 puntos menos en MSE que el primero. Además, su R^2 fue mejor, con un valor de 0.85 sobre 0.83. Estos valores altos indican que puede existir overfitting, porque la red neuronal no es muy compleja y aun así se tiene un R^2 bastante alto, lo cual indica que la red neuronal puede estarse adecuando mucho a los datos.


## 12. Analice si no hay sobreajuste
### Modelo 1 analisis de sobreajuste

```{r}
    mse_train <- mean((prediction_1_train - train$SalePrice)^2)
    mse_test <- mean((prediction_1 - test$SalePrice)^2)

    train_errors <- (prediction_1_train - train$SalePrice)^2
    test_errors <- (prediction_1 - test$SalePrice)^2
    
    plot(seq_along(train_errors), train_errors, type="l", col="blue", ylim=c(0, 5),xlim=c(0,420), xlab="Observation", ylab="MSE")
    lines(seq_along(test_errors), test_errors, col="red")
    legend("topright", legend=c("Train", "Test"), col=c("blue", "red"), lty=1)


    folds <- createFolds(train$SalePrice,k = 10, list = TRUE, returnTrain = TRUE)
    
    rmse_train <- vector(length = length(folds))
    rmse_test <- vector(length = length(folds))

   
    for (i in 1:length(folds)){
        train_data <- train[folds[[i]],]
        test_data <- train[-folds[[i]],]

        pred_train<-round(predict(model1,newdata = train_data ),0)
        pred<-round(predict(model1,newdata = test_data),0)

        rmse_train[i] <- RMSE(pred_train, train_data$SalePrice)
          rmse_test[i] <- RMSE(pred, test_data$SalePrice)
      }

      rmse_df <- data.frame(
    Fold = 1:length(folds),
  RMSE_Train = rmse_train,
    RMSE_Test = rmse_test
 ) 

ggplot(rmse_df, aes(x = Fold)) +
  geom_line(aes(y = RMSE_Train, color = "Conjunto de training")) +
    geom_line(aes(y = RMSE_Test, color = "Conjunto de test")) +
    labs(title = "Valores de RMSE en la validación cruzada",
         x = "Fold",
       y = "RMSE") +
  scale_color_manual(values = c("Conjunto de training" = "blue", "Conjunto de test" = "red")) +
  theme_minimal()
```
  
  
  ###   Modelo 2 analisis de sobreajuste
    
```{r}
      mse_train <- mean((prediction_2_train - train$SalePrice)^2)
      mse_test <- mean((prediction_2 - test$SalePrice)^2)
  
    train_errors <- (prediction_2_train - train$SalePrice)^2
    test_errors <- (prediction_2 - test$SalePrice)^2

    plot(seq_along(train_errors), train_errors, type="l", col="blue", ylim=c(0, 5),xlim=c(0,420), xlab="Observation", ylab="MSE")
    lines(seq_along(test_errors), test_errors, col="red")
    legend("topright", legend=c("Train", "Test"), col=c("blue", "red"), lty=1)


    folds <- createFolds(train$SalePrice,k = 10, list = TRUE, returnTrain = TRUE)
    
    rmse_train <- vector(length = length(folds))
    rmse_test <- vector(length = length(folds))

   
    for (i in 1:length(folds)){
        train_data <- train[folds[[i]],]
        test_data <- train[-folds[[i]],]

        pred_train<-round(predict(model2,newdata = train_data ),0)
        pred<-round(predict(model2,newdata = test_data),0)

        rmse_train[i] <- RMSE(pred_train, train_data$SalePrice)
        rmse_test[i] <- RMSE(pred, test_data$SalePrice)
    }
  
      rmse_df <- data.frame(
  Fold = 1:length(folds),
    RMSE_Train = rmse_train,
    RMSE_Test = rmse_test
)
  
  ggplot(rmse_df, aes(x = Fold)) +
  geom_line(aes(y = RMSE_Train, color = "Conjunto de training")) +
  geom_line(aes(y = RMSE_Test, color = "Conjunto de test")) +
  labs(title = "Valores de RMSE en la validación cruzada",
         x = "Fold",
         y = "RMSE") +
    scale_color_manual(values = c("Conjunto de trainig" = "blue", "Conjunto de test" = "red")) +
  theme_minimal()
```

 En  las curvas de aprendijaze que se muestran anteriormente se puede notar que tanto para test como para training no existe un comportamiento normal de la curva, por lo que se puede decir que si existe sobreajuste ambos modelos. Esto se puede deber a que la red neuronal no es muy compleja y aun así se tiene un R^2 bastante alto, lo cual indica que la red neuronal puede estarse adecuando mucho a los datos.
  
  ## 13.Discuta si puede mejorar el modelo.
A pe    sar de haber implementado técnicas para evitar el sobreajuste en los modelos creados, tales como el ajuste de parámetros y el uso de validación cruzada con diferentes funciones de activación, así como el parámetro de dropout, estos aún presentan dicho problema. Lamentablemente, ninguna de estas técnicas ha logrado mejorar los modelos, por lo que no es factible mejorar ninguno de ellos.
    
## 1    4. Compare la eficiencia
El   modelo 1 de RNA se tardo `r pm1$sampling.time` ms, mientras que el modelo 2 se tardó `r pm2$sampling.time` ms.
  
##  # Random Forest
#
#
#
#
#
#
#
#
#
#
#
El modelo logró un accuracy de 0.8269 y un tiempo de procesamiento de 0.82 ms. Además, obtuvo una sensitivity de 0.8947 y una specificity de 0.9443, lo que sugiere que el modelo tiene un bajo índice de errores.

### Arboles de Decision
El modelo logró un accuracy de 0.7312 y un tiempo de procesamiento de 0.95 ms. Asimismo, obtuvo una sensitivity de 0.7730 y una specificity de 0.9058. Estos resultados sugieren que los errores del modelo se deben principalmente a la clasificación incorrecta de verdaderos positivos.

### Naive bayes
El modelo logró un accuracy de 0.7472 y un tiempo de procesamiento de 0.64 ms. Además, obtuvo una sensitivity de 0.8355 y una specificity de 0.9582, lo que sugiere que el modelo tiene un bajo índice de errores, con algunas excepciones en casos de falsos negativos.

### SVM
El modelo logró un accuracy de 0.7173 y un tiempo de procesamiento de 0.72 ms. Además, obtuvo una sensitivity de 0.7368 y una specificity de 0.8837, lo que sugiere que el modelo tiene dificultades para identificar los verdaderos positivos.

## 15. Comparar los resultados del mejor modelo de esta hoja para clasificar con los resultados de los algoritmos usados para clasificar de las hojas de trabajo anteriores

Para el modelo de RNA:
  - accuracy de 0.7768
  - sensitivity de 0.8553
  - specificity de 0.9408

Para el modelo de Random Forest:
  - accuracy de 0.8269
  - sensitivity de 0.8947
  - specificity de 0.9443

Para el modelo de Arboles de Decision:
  - accuracy de 0.7312
  - sensitivity de 0.7730
  - specificity de 0.9082

Para el modelo de Naive Bayes:
  - accuracy de 0.7472
  - sensitivity de 0.8355
  - specificity de 0.9582

Para el modelo de SVM:
  - accuracy de 0.7173
  - sensitivity de 0.7368
  - specificity de 0.8837

En base a la métrica de accuracy, el modelo de Random Forest con un valor de 0.8269 es el mejor modelo. Además, este modelo también muestra los valores más altos de especificidad y sensibilidad. En comparación, el modelo de RNA obtuvo una exactitud de 0.7768, por lo que no es el mejor modelo para la clasificación y no parece ser mejor que los modelos previamente creados.

### 16. Comparar los resultados del mejor modelo para predecir el precio de venta con los resultados de los algoritmos usados para el mismo propósito de las hojas de trabajo anteriores.

Para el modelo de regresión lineal simple:
  - r-cuadrado de 0.61 
  - RMSE de 0.58

Para el modelo de regresión lineal múltiple:
  - r-cuadrado de 0.75
  - RMSE de 0.42

Para el modelo de SVM:
  - r-cuadrado de 0.81
  - RMSE de 0.41

Para el modelo de arbol de regresión:
  - r-cuadrado de 0.70
  - RMSE de 0.52

Para el modelo actual de RNA:
  - r-cuadrado de 0.49
  - RMSE de 0.37


Se puede observar que el mejor modelo para predecir el precio de venta es el modelo de SVM, ya que tiene un valor de R-cuadrado de 0.81 y un RMSE de 0.41. En comparación, el modelo de RNA tiene un valor de R-cuadrado de 0.49 y un RMSE de 0.37, lo que sugiere que no es un buen modelo para predecir el precio de venta.

### 17. Ahora que ha usado todos los modelos que hemos visto y aplicados al conjunto de datos llegue a conclusiones sobre cual es o cuales son los mejores modelos para clasificar dadas las características del conjunto de datos. ¿Cuál o cuáles son los mejores para predecir el precio de las casas? Una tabla de resumen con las métricas de los modelos le puede resultar muy útil para esto.

Para el modelo de regresión lineal simple:
  - r-cuadrado de 0.61 
  - RMSE de 0.58

Para el modelo de regresión lineal múltiple:
  - r-cuadrado de 0.75
  - RMSE de 0.42

Para el modelo de SVM:
  - r-cuadrado de 0.81
  - RMSE de 0.41

Para el modelo de arbol de regresión:
  - r-cuadrado de 0.70
  - RMSE de 0.52

Para el modelo actual de RNA:
  - r-cuadrado de 0.49
  - RMSE de 0.37

Basándonos en el análisis de los resultados obtenidos, podemos concluir que el modelo de SVM es el mejor para predecir el precio de venta, ya que presenta el valor más alto de R-cuadrado y un RMSE más bajo en comparación con los otros modelos evaluados. El modelo de regresión lineal múltiple también presenta un buen desempeño, aunque no tan alto como el modelo de SVM. Es importante tener en cuenta que la elección del mejor modelo depende de diferentes factores, como la complejidad del modelo y la interpretación de las variables, por lo que se recomienda considerar cuidadosamente los resultados y seleccionar el modelo que mejor se ajuste a las necesidades específicas del problema en cuestión.
